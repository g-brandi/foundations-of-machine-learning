{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 5. Neural networks for classification**\n",
    "This lab's primary purpose is to explain the process of building, training and evaluating a simple deep neural network using the California housing dataset. Our goal is to predict whether the median house value in a given California neighborhood is high or not, using a threshold-based approach to create a binary classification problem.\n",
    "\n",
    "## **1. Preparing the dataset**\n",
    "As usal, let's start by setupping the necessary libraries and utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create two dataframes `df_train` and `df_test` as the distinction between the training set and the test set is made at the dataset level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df_train = pd.read_csv(\"dataset_train.csv\")\n",
    "df_test = pd.read_csv(\"dataset_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Dataset shuffling**\n",
    "In order to remove any unintended patterns or biases in the data, we randomize the order of samples by shuffling the dataset. We can do that by creating a randomly permutated sequence of indexes based on the number of rows in the `df_train` dataframe and then by reordering the rows of the dataframe according to the randomly permuted indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data shuffling\n",
    "df_train = df_train.reindex(np.random.permutation(df_train.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2. Dataset normalization**\n",
    "Then we compute the mean and the standard deviation of the training set in order to perform z-score normalization. This ensures that all features are on a common scale, with a mean of zero, leading to more efficient and effective training of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the z-scores\n",
    "df_train_mean = df_train.mean()\n",
    "df_train_std = df_train.std()\n",
    "\n",
    "# Performing z-score normalization\n",
    "df_train_norm = (df_train - df_train_mean)/df_train_std\n",
    "df_test_norm = (df_test - df_train_mean)/df_train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we need to use the scores computed on the training set to normalize also the test set. Let's use the `head()` method to inspect the first few rows of `df_train_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>0.654364</td>\n",
       "      <td>-0.811862</td>\n",
       "      <td>0.588757</td>\n",
       "      <td>-0.512244</td>\n",
       "      <td>-0.546646</td>\n",
       "      <td>-0.240078</td>\n",
       "      <td>-0.577919</td>\n",
       "      <td>-0.300855</td>\n",
       "      <td>-0.877717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16653</th>\n",
       "      <td>-1.589839</td>\n",
       "      <td>1.345025</td>\n",
       "      <td>-1.000192</td>\n",
       "      <td>0.712098</td>\n",
       "      <td>0.233901</td>\n",
       "      <td>0.246918</td>\n",
       "      <td>0.295896</td>\n",
       "      <td>0.820384</td>\n",
       "      <td>0.386253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16574</th>\n",
       "      <td>-1.559916</td>\n",
       "      <td>1.260808</td>\n",
       "      <td>-1.079639</td>\n",
       "      <td>-0.049389</td>\n",
       "      <td>-0.297535</td>\n",
       "      <td>-0.025765</td>\n",
       "      <td>-0.195625</td>\n",
       "      <td>0.931644</td>\n",
       "      <td>-0.300912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10673</th>\n",
       "      <td>-0.502647</td>\n",
       "      <td>0.806973</td>\n",
       "      <td>-0.841297</td>\n",
       "      <td>-0.889776</td>\n",
       "      <td>-0.862186</td>\n",
       "      <td>-0.792413</td>\n",
       "      <td>-0.902999</td>\n",
       "      <td>-0.461481</td>\n",
       "      <td>-0.913929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14369</th>\n",
       "      <td>-1.270663</td>\n",
       "      <td>0.942656</td>\n",
       "      <td>-1.635772</td>\n",
       "      <td>0.435027</td>\n",
       "      <td>0.734495</td>\n",
       "      <td>0.415930</td>\n",
       "      <td>0.636579</td>\n",
       "      <td>-0.111510</td>\n",
       "      <td>-0.081054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "6331    0.654364 -0.811862            0.588757    -0.512244       -0.546646   \n",
       "16653  -1.589839  1.345025           -1.000192     0.712098        0.233901   \n",
       "16574  -1.559916  1.260808           -1.079639    -0.049389       -0.297535   \n",
       "10673  -0.502647  0.806973           -0.841297    -0.889776       -0.862186   \n",
       "14369  -1.270663  0.942656           -1.635772     0.435027        0.734495   \n",
       "\n",
       "       population  households  median_income  median_house_value  \n",
       "6331    -0.240078   -0.577919      -0.300855           -0.877717  \n",
       "16653    0.246918    0.295896       0.820384            0.386253  \n",
       "16574   -0.025765   -0.195625       0.931644           -0.300912  \n",
       "10673   -0.792413   -0.902999      -0.461481           -0.913929  \n",
       "14369    0.415930    0.636579      -0.111510           -0.081054  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. Creating binary labels**\n",
    "È un dataset pensato per la regressione perché come outcome vogliamo predire il prezzo delle case. Si può trasformare in un problema di classificazione su base statistica, cioè fissando una trheshold statistica. Consideriamo per le nostre tasche un prezzo alto classificazione personalizzata fissare  come prezzo elevato il 75esimo percentile signiica mettersi gia`nelle code quindi un prezzo al di sopra della media. Dividiamo ragionevolmente la label del dataset in uni e zeri dove uno è prezzo alto e zero è prezzo norale. Infatti in questo caso per dar senso ad 1 e 0 il nome della label è mhis high. Come vediamo è stato binarizzato dai valori regressivi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265000.0\n"
     ]
    }
   ],
   "source": [
    "# 75th percentile of median house value\n",
    "threshold = df_train['median_house_value'].quantile(q = 0.75)\n",
    "print(threshold)\n",
    "\n",
    "# Creating labels\n",
    "df_train_norm['median_house_value_is_high'] = (df_train['median_house_value'] > threshold).astype(float)\n",
    "df_test_norm['median_house_value_is_high'] = (df_test['median_house_value'] > threshold).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the results of our labelling by using again the `head()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6331     0.0\n",
       "16653    0.0\n",
       "16574    0.0\n",
       "10673    0.0\n",
       "14369    0.0\n",
       "Name: median_house_value_is_high, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_norm['median_house_value_is_high'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Representing data**\n",
    "Let's define an empty list that will eventually hold all the feature columns, keeping in mind that the features we are interested in to predict whether the median house value is high or not (`median_house_value_is_high`) are `latitude`, `longitude`, `median_income` and `population`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list\n",
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before training the model, we need to perform some preprocessing and features engineering techniques: bucketing and feature crossing.\n",
    "\n",
    "### **2.1. Bucketing (discretization)**\n",
    "Latitude and longitude are continuous values, but we want to simplify them and protect privacy. Instead of using precise geographical coordinates, we groud these values into categories known as buckets. Imagine dividing the geographic area into a grid where each cell represents a range of latitude and longitude. We choose a resolution equal to 30% of the standard deviation of the normalized data. This means each bucket covers a section of the geographic area that corresponds to the 30% of the standard deviation.\n",
    "\n",
    "In practice, this involves creating regular intervals for latitude and longitude, turning them into categories that represent broader, less detailed geographic areas.\n",
    "\n",
    "Bucketing is a technique there continuous numerical values are converted into discrete intervals or categories called buckets. This can simplify the model's task and reduce the noise from minute variations in the data. As we said, for the features `latitude` and `longitude`, we group their values into ranges rather than using their precise values.\n",
    "\n",
    "First we set `resolution_in_Zs` which determines the size of each bucket. Initially we scaled all the columns, including `latitude` and `longitude`, into their z-scores. So, instead of picking a resolution in degrees, we use a resolution in terms of z-scores. A `resolution_in_Zs` equal to 1 corresponds to a full standard deviation. In our case it's set to 30%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolution\n",
    "resolution_in_Zs = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start by treating `latitude` as a numeric column, enabling `tensorflow` to handle it. After that we create boundaries for the buckets by generating a range of values from the minimum to the maximum latitude in the training set. Each step in this range is 0.3 of the standard deviation of the normalized data. For example, if the range of our latitude is from -2 to 2 and the standard deviation is 1, this would create buckets at -2, -1.7, -1.4 and so on. Finally, we create the bucketized columns using these boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\giuse\\AppData\\Local\\Temp\\ipykernel_31704\\4274144216.py:2: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n",
      "WARNING:tensorflow:From C:\\Users\\giuse\\AppData\\Local\\Temp\\ipykernel_31704\\4274144216.py:6: bucketized_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n"
     ]
    }
   ],
   "source": [
    "# Creating a bucket feature column for latitude\n",
    "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
    "latitude_boundaries = list(np.arange(int(min(df_train_norm['latitude'])), \n",
    "                                     int(max(df_train_norm['latitude'])), \n",
    "                                     resolution_in_Zs))\n",
    "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same thing for `longitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a bucket feature column for longitude\n",
    "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
    "longitude_boundaries = list(np.arange(int(min(df_train_norm['longitude'])), \n",
    "                                      int(max(df_train_norm['longitude'])), \n",
    "                                      resolution_in_Zs))\n",
    "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, longitude_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Feature crossing**\n",
    "After creating the buckets for latitude and longitude, we don't treat them as independent columns. Instead, we combine these buckets into a new feature that represents the geographic location. This process is called feature crossing. Rathere than considering `latitude` and `longitude` separately, we combine them to form a single feature that captures the interaction between these two variabes. This combination allows us to handle geographic position as a unified entity rather than as two separate measurements, making it easier for the model to understand how location affects house prices. \n",
    "\n",
    "For our purpose, we use the bucketized columns to create the crossed feature `latitude_x_longitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\giuse\\AppData\\Local\\Temp\\ipykernel_31704\\1021138455.py:2: crossed_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.experimental.preprocessing.HashedCrossing` instead for feature crossing when preprocessing data to train a Keras model.\n"
     ]
    }
   ],
   "source": [
    "# Crossing features\n",
    "latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the `hash_bucket_size` parameter specifies the number of hash buckets used to manage the crossed feature. Hashing is used to efficiently map the combination of the buckets into a fixed number of categories. This helps to handle potentially large combinations of latitude and longitude buckets without creating an unwieldy number of features.\n",
    "\n",
    "Once we have the crossed feature, we create an indicator column `crossed_feature`. This represents the crossed feature as a one-hot necoded vector, which is suitable for feeding into our model. Finally, we add this crossed feature to our previously empty-delcared list `feature_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\giuse\\AppData\\Local\\Temp\\ipykernel_31704\\1281448938.py:2: indicator_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n"
     ]
    }
   ],
   "source": [
    "# Adding the crossed features to the list\n",
    "crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n",
    "feature_columns.append(crossed_feature)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same thing for the remaining features `median_income` and `population`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding median income to the list\n",
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "feature_columns.append(median_income)\n",
    "\n",
    "# Adding population to the list\n",
    "population = tf.feature_column.numeric_column(\"population\")\n",
    "feature_columns.append(population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our list of features, we convert it into a single dense tensor that can be fed into our deep neural network. This step is crucial because neural networks expect their input to be in the form of a dense matrix (or tensor), where each input feature is represented as a continuous value or a one-hot encoded vector.\n",
    "\n",
    "We use the `DenseFeatures` layer from `keras` to combine all the features into a single dense tensor. This tensor is what the neural network will use as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras._tf_keras.keras.layers' has no attribute 'DenseFeatures'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Converting the list of features into a layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m my_feature_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseFeatures\u001b[49m(feature_columns)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras._tf_keras.keras.layers' has no attribute 'DenseFeatures'"
     ]
    }
   ],
   "source": [
    "# Converting the list of features into a layer\n",
    "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Building the neural network**\n",
    "We can finally build our neural network.\n",
    "\n",
    "### **3.1. Defining functions**\n",
    "Since we do not have a built-in function for visualization, we need to create one called. One crucial plot we require is the curve showing the evolution of a seres of metrics over the epochs (as neural networks are dynamic). Tipically we might use log loss or cross-entropy loss.\n",
    "\n",
    "Our `plot_curve()` function will take as an input the number of epochs, the history of the model and the list of metrics to be considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the plotting function\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "    \"\"\"Plot a curve of one or more classification metrics vs epoch.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Value')\n",
    "\n",
    "    for m in list_of_metrics:\n",
    "        x = hist[m]\n",
    "        plt.plot(epochs[1:], x[1:], label = m)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a function that allows us to create the model. Our `create_model()` function will take as an input the learning rate, the previously built input layer `feature_layer` and the list of metrics to be considered. We first initialize our model through the `Sequential()` method from the `models` propoerty of `keras`.\n",
    "\n",
    "Then we add the input layer and after that we add a single dense layer with the sigmoid function as the activation function. In `keras` it is quite straightforward to specialize the model because we call the `add()` method each time we need to specify a certain characteristic.\n",
    "\n",
    "Finally we compile the model. We need to specify the optimizer, the loss and the evaluation metrics. The only loss available for binary classification is `BinaryCrossentropy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "def create_model(my_learning_rate, feature_layer ,my_metrics):\n",
    "    \"\"\"\"Create and compile a simple neural network model.\"\"\"\n",
    "\n",
    "    # Simple model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Adding the layer containing the feature columns\n",
    "    model.add(feature_layer)\n",
    "\n",
    "    # Adding a single layer\n",
    "    model.add(tf.keras.layers.Dense(units = 1, input_shape = (1,), activation = tf.sigmoid),)\n",
    "\n",
    "    # Building the layers into a model tensorflow can execute\n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(lr = my_learning_rate), loss = tf.keras.losses.BinaryCrossentropy(), metrics = my_metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network model, it's good practice to define a structured training function rather than using inline scripting. Training neural networks is more complex than other algorithms and ofter requires more than just one line of code. Our `train_model()` function will take as an input the model created using the previous function, the training dataset, the number of epochs, the batch size (the number of samples used in each training batch) and the label name (the target output).\n",
    "\n",
    "In `tensorflow`, the training dataset needs to be split into features and labels. Unlike other libraries that accept dataframes structures directly, `tensorflow` requires separate structures for the features and the labels. Therefore, we need to extract the features and the label from the training dataset.\n",
    "\n",
    "When fitting the model, we want to keep track of the training history. The fitting process uses the feas the features as the input and the label column as the output. Additional parameters include the number of epochs and the batch size. We also set the property `shuffle` to true, meaning that the data rows will be shuffled again before creating the batches for training. To extract the epoch information, we access the `epoch` attribute of the `history` object returned by the fitting porcess.\n",
    "\n",
    "Since we prefer working with `pandas` dataframes for visualization, we convert the `history` dictionary into the dataframe `hist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function\n",
    "def train_model(model, dataset, epochs, batch_size, label_name):\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # Splitting dataset into features and label\n",
    "    features = {name:np.array(value) for name, value in dataset.items()}\n",
    "    label = np.array(features.pop(label_name))\n",
    "\n",
    "    # Storing fitting results\n",
    "    history = model.fit(x = features, y = label, batch_size = batch_size, epochs = epochs, shuffle = True)\n",
    "\n",
    "    # Getting useful details for plotting the loss curve\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # Converting the history dictionary to a pandas dataframe\n",
    "    hist = pd.DataFrame(history.history)\n",
    "\n",
    "    return epochs, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Defining parameters and metrics**\n",
    "When training the neural network, several parameters need to be specified. Grid search is a common method for tuning these parameters, where we define a grid of possible values by specifying ranges on the x and y axes, creating a space of potential combinations. However, for simplicity and to avoid prolonged training times, we fix these parameters such as the learning rate, the number of epochs and the bath size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also specify the label of the outcome we are interested in and the classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = 'median_house_value_is_high'\n",
    "classification_threshold = 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the list of metrics we are interested in, in order to evaluate the model from different points of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metrics\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(name = 'accuracy', threshold = classification_threshold),\n",
    "           tf.keras.metrics.Precision(name = 'precision', thresholds = classification_threshold),\n",
    "           tf.keras.metrics.Recall(name = 'recall', thresholds = classification_threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Training the model**\n",
    "We can finally implement the model using our `create_model()` function and then train it using our `train_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the model\n",
    "my_model = create_model(learning_rate, my_feature_layer, metrics)\n",
    "\n",
    "# Training the model\n",
    "epochs, hist = train_model(my_model, df_train_norm, epochs, batch_size, label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the metrics using our `plot_curve()` function. As we can see, precision and recall set after a certain number of epochs, while accuracy becomes a plateau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating metrics\n",
    "list_of_metrics_to_plot = ['accuracy', 'precision', 'recall']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Testing model**\n",
    "Finally we certify our trained model. Again, we build two separate structures for the features and for the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = {name:np.array(value) for name, value in df_test_norm.items()}\n",
    "test_label = np.array(test_features.pop(label_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we evaluate our model on the test set with the same parameters used during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.evaluate(x = test_features, y = test_label, batch_size = batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
